#!/bin/bash

# Run this script for each batch separately

# HOW2RUN: qsub DSGPfilter1.sh -F "X Y Z"
# Where X is the batch number, Y is the first decimal value of the INFO_SCORE filter (e.g., 4, 6), and Z is the GP filter value (e.g., 0.99 or 0.9)

# Note that the script only works if the input file follows the format batch${batch}.vcf.gz, which is generated by the script 1_PrepareData.sbatch and stored in the "batches" folder

# Change to the directory where the job was submitted to ensure relative paths work correctly
cd $SLURM_SUBMIT_DIR

###
# VARIABLES
###

batch=$1
IS=$2
GP=$3

# Store the current working directory in the variable 'path'
path=$PWD

# Make sure script DSGPfilter.pl is in folder scripts
scripts="${path}/scripts"

num_cores=8
metrics=batch${batch}_metrics.txt
input_file_basename=$(basename "${metrics}" .txt)
metrics_IS="${input_file_basename}.IS0.${IS}.txt"

# 1 # Extract variants with MAF>0.05;  fix GT/GP;  Keep only specific fields for each sample for downstream analyses

# The generated files, regardless of the batch being processed, contain all sites with MAF > 0.05, meaning they include the exact same number of sites.

## OUTPUT: The output file (e.g., batch1_metrics.txt) contains as many columns as the number of individuals in the VCF file, with GT, GP, and DS information for each variant. Note: No new VCF file is generated.

current_batch="${path}/batches/batch${batch}"
mkdir -p $current_batch &&
mv ${path}/batches/batch${batch}.vcf.gz* $current_batch &&
path2="${current_batch}/metrics" &&
mkdir -p $path2 &&
echo "Run parameters: batch $batch IS $IS GP $GP" > ${current_batch}/batch${batch}.${IS}.${GP}.log
bcftools view -T ${path}/maf/HRCmaf0.05sites_4bcftools.txt ${current_batch}/batch${batch}.vcf.gz --threads 8 -Ou | bcftools +tag2tag --threads 8 -Ou -- -t 1 --gp-to-gt | bcftools query -f '%CHROM\_%POS\_%REF\_%ALT[\t%GT\_%GP\_%DS]\n' > ${path2}/${metrics} &&

# 2 # INFO SCORE FILTER

# From the previously generated files (e.g., batch1_metrics.txt), sites with an INFO SCORE below a specified threshold (e.g., 0.4 or 0.6) are removed. The removed sites are those previously identified using remove.variants.R.

# As a result, each output file may have a different number of columns (i.e., the number of samples, which depends on the batch being processed and whether batches contain a different number of samples; otherwise, they are identical), but the number of rows (i.e., sites) remains the same. This is because all batches start with the same number of unfiltered sites, and the same number of sites is removed from each. In summary, all post-filter files contain the same number of sites.

## OUTPUT: The resulting file is the same as before (e.g., batch1_metrics.txt), but without sites with an INFO SCORE below the specified threshold. For example, the output file may be named batch1.IS0.4.txt.

tail -n +2 ${path}/INFOSCOREbyBatch/IDs2remove.txt | cut -f1 > ${path}/INFOSCOREbyBatch/remove-IS0.${IS}.txt &&

# split input files into parts

## Often, we are dealing with a large file containing many samples, so we split it into smaller files to process it more efficiently.

# Split the input file into smaller parts, each containing one-tenth of the total number of lines, for more efficient processing.

split --lines=$(expr $(wc -l < "${path2}/${metrics}") / 10) ${path2}/${metrics} ${path2}/${input_file_basename}.${IS}.part &&

# The AWK command stored in the variable awk_script processes two input files sequentially. During the first file's reading (FNR==NR condition), it builds a set of unique values from the first column. When processing the second file, it outputs entire rows to a new file (FILENAME.out) only if the value in the first column is not present in the previously stored set from the first file.

awk_script='FNR==NR {a[$1]; next} !($1 in a) {print $0 > FILENAME".out"}'

# process each part in parallel

find "${path2}" -type f -name "${input_file_basename}.${IS}.part*" | parallel "awk '$awk_script' ${path}/INFOSCOREbyBatch/remove-IS0.${IS}.txt {}" &&

# concatenate output files into single output file

cat ${path2}/${input_file_basename}.${IS}.part*.out > ${path2}/${metrics_IS} &&

# cleanup temporary files

rm ${path2}/${input_file_basename}.${IS}.part* &&outpath="${path}/batches/MetricsBySample" &&
mkdir -p $outpath

# 3 # Split by sample (creates one file per sample with variant ID and metrics)

# The file batch*.IS0.4.txt is split by individual (essentially, the columns are separated)

folder=${outpath}/batch${batch}.IS0.${IS}
mkdir -p $folder

# Extract number of samples in current batch

num=$(bcftools query -l ${current_batch}/batch${batch}.vcf.gz | wc -l)

# Create output files

for ((j=1; j<=$num; j++)); do
filename="${folder}/Sample${j}.${batch}.${IS}.txt"
if [[ ! -s $filename ]]; then
touch $filename
fi
done

# Check if all files have correct length

# Check the value of IS and set ref_length
# Store the length of ${path2}/${metrics} because it corresponds exactly to the number of variants with MAF>0.05 found in the imputed files

reflength=$(wc -l < ${path2}/${metrics})
IS_removed=$(wc -l < ${path}/INFOSCOREbyBatch/remove-IS0.${IS}.txt)
expectedSites=$((reflength - IS_removed))
ref_length=$expectedSites &&

# Print the value for $expectedSites

echo "For INFO SCORE 0.$IS, the expected number of sites is $expectedSites" >> ${outpath}/batch${batch}.${IS}.${GP}.log &&

# Extract and modify columns for each file

# As structured, this approach is useful if a job is interrupted for any reason, as it allows the process to resume from where it left off. Specifically, it checks if the file to be created already exists: if it doesn't exist, it starts from scratch. If the file already exists, two options are considered: 1) The file has the expected length, meaning it is completed. 2) The file is not complete, so the code identifies the last written line, removes it (as it may be incomplete), and continues writing from that point onward. However, this approach has limitations, and using a parallel method might be more efficient.

for ((col=2; col<=($num+1); col++)); do
outname="${folder}/Sample$((col-1)).${batch}.${IS}.txt"
if [[ ! -s $outname ]]; then
awk -v OFS="\t" -v col=$col '{{gsub(/[_|,]/,"\t",$col); print $1,$col}}' "${path2}/${metrics_IS}" >> "${outname}" &
else
length=$(wc -l < "$outname")
if [[ $length -eq $expectedSites ]]; then
echo "Sample$((col-1)) is completed" >> ${outpath}/batch${batch}.${IS}.${GP}.log
else
start_line=$(awk 'END{print NR}' "$outname")
echo "Sample$((col-1)) restarts from line $start_line" >> ${outpath}/batch${batch}.${IS}.${GP}.log
sed -i "${start_line}d" "$outname"
awk -v OFS="\t" -v col=$col -v start_line=$start_line '{{gsub(/[_|,]/,"\t",$col); if (NR>=start_line) print $1,$col}}' "${path2}/${metrics_IS}" >> "${outname}" &
fi
fi
done
# wait for the current batch of jobs to finish before starting the next one
wait
# Wait until all files have been written
wait

# Verifies if the files created by the previous block have been correctly written with the expected length
seq=$(seq 1 $num)
find ${folder} -maxdepth 1 -name "Sample*.${batch}.${IS}.txt" -print0 | parallel -0 -j 10 "filename=${folder}/Sample{}.${batch}.${IS}.txt; len=\$(awk 'END{printNR}' \"\${filename}\"); if [[ \$len -ne $expectedSites ]]; then echo 'Error: The file Sample{}.${batch}.${IS}.txt has a different length than ${path2}/${metrics_IS}.'; fi" ::: $seq &&

# 4 # Filter variants per sample and creates for each one of them a list of variants to keep and one of variants to remove.

keep_folder=${folder}/keep.GP${GP}
remove_folder=${folder}/remove.GP${GP}
mkdir -p $keep_folder $remove_folder
seq=$(seq 1 $num)
parallel -j $num_cores --delay 2 "f=${folder}/Sample{}.${batch}.${IS}.txt;keep=${keep_folder}/Sample{}.${batch}.${IS}.${GP}.keep; remove=${remove_folder}/Sample{}.${batch}.${IS}.${GP}.remove; GP=${GP}; perl ${scripts}/DSGPfilter.pl \$f \$keep \$remove \$GP" ::: $seq &&

# 5 # Create a file with Sample name, count of variants to keep, count of variants to remove, and (ratio variants to remove) / (total variants)

# Find all file with name Sample*.keep in current directory

for keepfile in ${keep_folder}/Sample*.${GP}.keep; do

# Extract the sample name from the file Sample*.keep.

sample=$(echo "$keepfile" | sed 's/\.keep//')
sample_basename=$(basename $sample)

# Initialize the output variables

removesize=0
ratio=0

# Calculate the size of the file Sample*.keep

keepsize=$(wc -l < "$keepfile")

# Find the corresponding file Sample*.remove

removefile="${remove_folder}/${sample_basename}.remove"

# Calculate the size of the file Sample*.remove

removesize=$(wc -l < "$removefile")

# Calculate the ratio between the size of the file Sample.remove and the sum of the sizes of Sample.keep and Sample*.remove

total=$(($keepsize + $removesize))
ratio=$(echo "scale=2; $removesize / $total" | bc)

# Print risults

echo "$sample_basename $keepsize $removesize $ratio" >> ${folder}/Sample_LQV.${batch}.${IS}.${GP}.txt
done
echo "DSGPfilter1.pbs done! Now check the Sample_LQV list to find what samples have to be removed. Once decided, remove the corresponding *keep and*remove files from the dedicated folder and proceed with DSGPfilter2.pbs" >> ${outpath}/batch${batch}.${IS}.${GP}.log
